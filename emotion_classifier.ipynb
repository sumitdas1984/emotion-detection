{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,SpatialDropout1D,Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_name):\n",
    "    data_list  = []\n",
    "    with open(file_name,'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label = ' '.join(line[:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data_list.append([label, text])\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(text_list):\n",
    "    label_list = []\n",
    "    text_list = [text_list[i][0].replace('[','') for i in range(len(text_list))]\n",
    "    label_list = [list(np.fromstring(text_list[i], dtype=float, sep=' ')) for i in range(len(text_list))]\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_msgs(text_list):\n",
    "    msg_list = []\n",
    "    msg_list = [text_list[i][1] for i in range(len(text_list))]\n",
    "    return msg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_file):\n",
    "    with open(glove_file,'r',encoding='UTF-8') as file:\n",
    "        words = set()\n",
    "        word_to_vec = {}\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            line[0] = re.sub('[^a-zA-Z]', '', line[0])\n",
    "            if len(line[0]) > 0:\n",
    "                words.add(line[0])\n",
    "                try:\n",
    "                    word_to_vec[line[0]] = np.array(line[1:],dtype=np.float64)\n",
    "                except:\n",
    "                    print('Error has occured')\n",
    "                    print('-'*50)\n",
    "                    print(line[1:])\n",
    "\n",
    "        i = 1\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        for word in sorted(words):\n",
    "            word_to_index[word] = i\n",
    "            index_to_word[i] = word\n",
    "            i = i+1\n",
    "    return word_to_index,index_to_word,word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index,word_to_vec):\n",
    "    corpus_len = len(word_to_index) + 1\n",
    "    embed_dim = word_to_vec['word'].shape[0]\n",
    "\n",
    "    embed_matrix = np.zeros((corpus_len,embed_dim))\n",
    "\n",
    "    for word, index in word_to_index.items():\n",
    "        embed_matrix[index,:] = word_to_vec[word]\n",
    "\n",
    "    embedding_layer = Embedding(corpus_len, embed_dim)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([embed_matrix])\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape,embedding_layer):\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    embedding_layer =  embedding_layer\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    reg = L1L2(0.01, 0.01)\n",
    "\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True,bias_regularizer=reg,kernel_initializer='he_uniform'))(embeddings)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(64)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(7, activation='softmax')(X)\n",
    "    X =  Activation('softmax')(X)\n",
    "    model = Model(sentence_indices, X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 50, 50)            17090100  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 256)           183296    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 256)           1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 17,357,051\n",
      "Trainable params: 17,356,539\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "5984/5984 [==============================] - 100s 17ms/step - loss: 6.3889 - acc: 0.1688\n",
      "Training Accuracy: 0.2724\n",
      "Testing Accuracy:  0.2373\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    textlist = read_text_file(\"data/data.txt\")\n",
    "    label_list = extract_labels(textlist)\n",
    "    msg_list = extract_text_msgs(textlist)\n",
    "    word_to_index,index_to_word,word_to_vec = read_glove_vector('resources/glove.6B.50d.txt')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(msg_list, label_list,stratify = label_list,\\\n",
    "        test_size = 0.2, random_state = 123)\n",
    "    tk = Tokenizer(lower = True, filters='')\n",
    "    tk.fit_on_texts(msg_list)\n",
    "    train_tokenized = tk.texts_to_sequences(x_train)\n",
    "    test_tokenized = tk.texts_to_sequences(x_test)\n",
    "    maxlen = 50\n",
    "    X_train = pad_sequences(train_tokenized, maxlen = maxlen)\n",
    "    X_test = pad_sequences(test_tokenized, maxlen = maxlen)\n",
    "    if os.path.exists('models/tokenizer.pickle'):\n",
    "        os.remove('models/tokenizer.pickle')\n",
    "        with open('models/tokenizer.pickle', 'wb') as tokenizer:\n",
    "            pickle.dump(tk, tokenizer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    embedding_layer = create_embedding_layer(word_to_index,word_to_vec)\n",
    "    model = create_lstm_model((maxlen,),embedding_layer)\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, np.array(y_train), epochs = 1, batch_size = 32, shuffle=True)\n",
    "    model.save('models/emoji_model.h5')\n",
    "#     model = load_model('models/emoji_model.h5')\n",
    "    loss, accuracy = model.evaluate(X_train, np.array(y_train), verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(X_test, np.array(y_test), verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
